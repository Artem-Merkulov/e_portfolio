# Portfolio
#### Здесь собраны некоторые реализованные проекты 
Более подробное описание проектов в файлах README.md в директориях с проектами.
 |Наименование проекта |Описание проекта       |  Инструменты   |
|:-------------------- |:----------------------|:--------------|
|[Проектирование ETL-пайплайна для финтех-стартапа](https://github.com/Artem-Merkulov/e_portfolio/tree/main/DE/Проектирование%20ETL-пайплайна%20для%20финтех-стартапа)| Спроектирован ETL-пайплайн для анализа и визуализации бизнес-метрик на основе информации о курсах валют и проведённых транзакциях. | `SparkStructuredStreaming`, `AirFlow`, `Metabase`, `Docker`,   `Kafka`, `SQL`, `PostgreSQL`, `python`, `Vertica`  | - Потоковая обработка данных;   <br>- Создание AirFlow DAG's;   <br>- Построение пайплайна;   <br>- Создание dashboard.     | Stream Processing; Apache Spark Structured Streaming; Apache Kafka; Vertica; PostgreSQL; Metabase; | 
|[Создание DWH с использованием облачных технологий для агрегатора доставки еды](https://github.com/Artem-Merkulov/e_portfolio/tree/main/DE/Создание%20DWH%20с%20использованием%20облачных%20технологий%20для%20агрегатора%20доставки%20еды)| С помощью инструментов Yandex Cloud, созданы три микросервиса обработки данных, с помощью которых наполнено  DWH для агрегатора доставки еды. | `Yandex Cloud`, `Kubernetes`, `Redis`, `Docker`, `Kafka`, `SQL`, `PostgreSQL`, `python`, `Data Lens` |
|[Настройка потоковой обработки данных для агрегатора доставки еды](https://github.com/Artem-Merkulov/e_portfolio/tree/main/DE/Настройка%20потоковой%20обработки%20данных%20для%20агрегатора%20доставки%20еды)| Создан сервис потоковой обработки данных, с помощью которого бизнес протестировал подписку на рестораны,  благодаря которой подписчики получили эксклюзивные акции на блюда ресторана. | `Kafka`, `SparkStructuredStreaming`, `PostgreSQL`, `python`, `pySpark` |
|[Обновление хранилища данных для соцсети](https://github.com/Artem-Merkulov/e_portfolio/tree/main/DE/Обновление%20хранилища%20данных%20для%20соцсети)|  Обновление структуры Data Lake соцсети.  Добавление витрины данных, в которых будет использоваться информация по координатам действий пользователей.  | `Hadoop`, `MapReduce`, `HDFS`, `Apache Spark`|
|[Поиск сообществ с высокой конверсией в первое сообщение](https://github.com/Artem-Merkulov/e_portfolio/tree/main/DE/Поиск%20сообществ%20с%20высокой%20конверсией%20в%20первое%20сообщение)| Перенос данных из хранилища транзакционного типа (строковой СУБД PostgreSQL) в хранилище аналитического типа (колоночная СУБД Vertica). | `python`, `SQL`, `PostgreSQL`, `Vertica`, `Airflow`, `S3` |
|[Реализация витрины для расчётов выплат курьерам](https://github.com/Artem-Merkulov/e_portfolio/tree/main/DE/Реализация%20витрины%20для%20расчётов%20выплат%20курьерам)| Рассчитаны суммы выплат курьерам: реализован DWH и ETL-пайплайн для данных.  | `python`, `REST-API`, `PostgreSQL`, `MongoDB`, `Airflow` |
|[Обновление пайплайна обработки данных](https://github.com/Artem-Merkulov/e_portfolio/tree/main/DE/Обновление%20пайплайна%20обработки%20данных)| Обновлён созданный пайплайн обработки данных для интернет-магазина: добавлены в витрину данные  по отмене заказов и возврату средств, а также рассчитаны метрики по «возвращаемости клиентов». |`python`, `SQL`, `PostgreSQL`, `Airflow`, `S3`|
|[Оптимизация модели данных интернет-магазина](https://github.com/Artem-Merkulov/e_portfolio/tree/main/DE/Оптимизация%20модели%20данных%20интернет-магазина)| Проведена миграция данных из большой и неудобной таблицы в отдельные логические таблицы. На их основе собраны витрины данных. | `python`, `SQL`, `PostgreSQL` |