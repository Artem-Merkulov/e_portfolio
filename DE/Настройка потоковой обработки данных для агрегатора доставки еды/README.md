# Настройка потоковой обработки данных для агрегатора доставки еды

#### Описание проекта:
* Создан сервис потоковой обработки данных, с помощью которого бизнес протестировал подписку на рестораны, 
благодаря которой подписчики получили эксклюзивные акции на блюда ресторана.

#### Задачи проекта:
Написать сервис, который будет:
1. Читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени.
2. Получать список подписчиков из базы данных Postgres. 
3. Джойнить данные из Kafka с данными из БД.
4. Сохранять в памяти полученные данные, чтобы не собирать их заново после отправки в Postgres или Kafka.
5. Отправлять выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане.
6. Вставлять записи в Postgres, чтобы получить фидбэк от пользователя. 

#### Инструменты: 
* `Kafka`, `SparkStructuredStreaming`, `PostgreSQL`, `python`, `pySpark`

#### Ключеные слова:
* Stream Processing; Apache Spark Structured Streaming; Apache Kafka; Consumer; Producer; Stream-Static Join;

#### Навыки:
- Построение системы потоковой обработки с использованием Apache Spark Structured Streaming;
- Работа с брокером сообщений Kafka; 
- Объединение потоковых и статических данных;
- Дедупликация данных при потоковой обработке.

#### Сфера деятельности:
* e-commerce

#### Файловая структура:
```
└───Настройка потоковой обработки данных для агрегатора доставки еды
    ├───PostgresScripts     - Код для работы с БД PostgreSQL
    ├───Push-Service
    │   ├───connections     - Настройки подключений для приложения push-service 
    │   └───etl             - SparkStructuredStreaming Jobs
    └───RandomKafkaProducer - Генератор данных для проверки работоспособности приложения
```